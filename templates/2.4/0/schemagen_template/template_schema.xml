<?xml version="1.0" encoding="UTF-8" ?>
<schema name="GSP2" version="1.4">
  <!--
	GSP service-template version: $Id: template_schema.xml 7160 2013-02-28 02:37:29Z hiragakazuak01 $
  --> 
  
  <!-- attribute "name" is the name of this schema and is only used for display purposes.
       Applications should change this to reflect the nature of the search collection.
       version="1.4" is Solr's version number for the schema syntax and semantics.  It should
       not normally be changed by applications.
       1.0: multiValued attribute did not exist, all fields are multiValued by nature
       1.1: multiValued attribute introduced, false by default 
       1.2: omitTermFreqAndPositions attribute introduced, true by default except for text fields.
       1.3: removed optional field compress feature
       1.4: default auto-phrase (QueryParser feature) to off
  -->
  <types>
    <!-- The StrField type is not analyzed, but indexed/stored verbatim. -->
    <fieldType name="string" class="solr.StrField" sortMissingLast="true" omitNorms="true"/>

    <!-- boolean type: "true" or "false" -->
    <fieldType name="bool" class="solr.BoolField" sortMissingLast="true" omitNorms="true"/>

    <!-- Numeric field types that -->
    <fieldType name="tint" class="solr.TrieIntField" precisionStep="4" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="4" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="4" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="4" omitNorms="true" positionIncrementGap="0"/>

    <!-- A Trie based date field for faster date range queries and date faceting. -->
    <fieldType name="tdate" class="solr.TrieDateField" omitNorms="true" precisionStep="6" positionIncrementGap="0"/>

    <!-- A text field that preserves the entire input string as a single token -->
    <fieldType name="text_kw" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>
        <tokenizer class="solr.KeywordTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.TrimFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- A text field that only splits on whitespace for exact matching of words -->
    <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- A text field that splits on pattern.
         This definition is a template for definition that is given by the user.
         User can specify the pattern for each field definition. So we will generate
         field type definition for pattern tokenizer that corresponds to the definition. 
    -->
    <fieldType name="text_del" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.PatternTokenizerFactory" pattern="/" />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Bi-gram -->
    <fieldType name="text_2gram" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <!-- Rakuten character normalization -->
        <charFilter class="solr.MappingCharFilterFactory"
                    mapping="rakuten-2gram-punctuation-mapping.txt"/>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>

        <!-- Special bigram tokenizer which adds end mark to end of value. -->
        <!-- You can specify  maxLength (default: 1024) -->
        <tokenizer class="jp.co.rakuten.gsp.solr.BigramIndexTokenizerFactory" />

        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <!-- Rakuten character normalization -->
        <charFilter class="solr.MappingCharFilterFactory"
                    mapping="rakuten-2gram-punctuation-mapping.txt"/>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>

        <!-- query side should not add end mark -->
        <tokenizer class="jp.co.rakuten.gsp.solr.BigramQueryTokenizerFactory"/>        

        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- CJK bigram -->
    <fieldType name="text_cjk" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>
        <!-- tokenizer class="solr.StandardTokenizerFactory"/ -->
        <!-- normalize width before bigram, as e.g. half-width dakuten combine  -->
        <!-- filter class="solr.CJKWidthFilterFactory"/ -->
        <!-- filter class="solr.JapaneseWidthFilterFactory"/ -->
        <!-- for any non-CJK -->
        <!--
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.CJKBigramFilterFactory"/>
        -->
        <!-- Before moving into Solr 4, we use the folliwing 
             configuration for CJK Bi-gram tokenization.
          -->
        <tokenizer class="solr.CJKTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!--
       Language dependent text field definitions
       autoGeneratePhraseQueries attributes is set to "false" as a default after version 1.3.
    -->

    <!-- Standard English -->
    <fieldType name="text_en_st" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_en.txt"
                enablePositionIncrements="true"
                />
        -->
        <filter class="solr.LowerCaseFilterFactory"/>
	    <filter class="solr.EnglishPossessiveFilterFactory"/>
        <!--
        <filter class="solr.KeywordMarkerFilterFactory"
                protected="lang/protwords.txt"/>
        -->
        <filter class="solr.PorterStemFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_en.txt"
                enablePositionIncrements="true"
                />
        -->
        <filter class="solr.LowerCaseFilterFactory"/>
	    <filter class="solr.EnglishPossessiveFilterFactory"/>
        <!--
        <filter class="solr.KeywordMarkerFilterFactory"
                protected="lang/protwords.txt"/>
        -->
        <filter class="solr.PorterStemFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- English -->
    <fieldType name="text_en" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.WordDelimiterFilterFactory"
                generateWordParts="1"
                generateNumberParts="1"
                catenateWords="1"
                catenateNumbers="0"
                catenateAll="0"
                preserveOriginal="0"
                />
        <filter class="solr.LowerCaseFilterFactory"/>
	    <filter class="solr.EnglishPossessiveFilterFactory"/>
        <filter class="solr.KStemFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.WordDelimiterFilterFactory"
                generateWordParts="1"
                generateNumberParts="1"
                catenateWords="0"
                catenateNumbers="0"
                catenateAll="0"
                preserveOriginal="0"
                />
        <filter class="solr.LowerCaseFilterFactory"/>
	    <filter class="solr.EnglishPossessiveFilterFactory"/>
        <filter class="solr.KStemFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- German -->
    <fieldType name="text_de" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_de.txt"
                format="snowball"
                enablePositionIncrements="true"/>
        -->
        <!-- filter class="solr.GermanNormalizationFilterFactory"/ -->
        <filter class="solr.GermanLightStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.GermanMinimalStemFilterFactory"/> -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Spanish -->
    <fieldType name="text_es" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_es.txt"
                format="snowball"
                enablePositionIncrements="true"/>
        -->
        <filter class="solr.SpanishLightStemFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- French -->
    <fieldType name="text_fr" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- removes l', etc -->
        <filter class="solr.ElisionFilterFactory"
                ignoreCase="true"
                articles="lang/contractions_fr.txt"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_fr.txt"
                format="snowball"
                enablePositionIncrements="true"/>
        -->
        <filter class="solr.FrenchLightStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.FrenchMinimalStemFilterFactory"/> -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

   <!-- Indonesian -->
    <fieldType name="text_id" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_id.txt"
                enablePositionIncrements="true"/>
        -->
        <!-- for a less aggressive approach (only inflectional suffixes),
             set stemDerivational to false -->
        <filter class="solr.IndonesianStemFilterFactory" stemDerivational="true"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>
 
   <!-- Portuguese -->
    <fieldType name="text_pt" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_pt.txt"
                format="snowball"
                enablePositionIncrements="true"/>
        -->
        <filter class="solr.PortugueseLightStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.PortugueseMinimalStemFilterFactory"/> -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

   <!-- Brazilian Portuguese -->
    <fieldType name="text_pt_br" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!--
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_pt.txt"
                format="snowball"
                enablePositionIncrements="true"/>
        -->
        <filter class="solr.BrazilianStemFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Simplified Chinese
         Currently we are using Bi-gram tokenizer to deal with Simplified Chinese in Phase-2.
    -->
    <fieldType name="text_zhs" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>
        <!--
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.JapaneseWidthFilterFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.CJKBigramFilterFactory"/>
        -->
        <tokenizer class="solr.CJKTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Traditional Chinese
         Currently we are using Bi-gram tokenizer to deal with Simplified Chinese in Phase-2.
    -->
    <fieldType name="text_zht" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>
        <!--
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.JapaneseWidthFilterFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.CJKBigramFilterFactory"/>
        -->
        <tokenizer class="solr.CJKTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Japanese -->
    <fieldType name="text_ja" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <!-- Rakuten character normalization -->
        <charFilter class="solr.MappingCharFilterFactory"
                    mapping="rakuten-universal-punctuation-mapping.txt"/>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>

        <!-- Japanese morphological analyzer -->
        <tokenizer class="solr.JapaneseTokenizerFactory"/>
        <!-- Apply width filter -->
        <filter class="solr.JapaneseWidthFilterFactory"/>

        <!-- Apply Kanji numeric normalizer (expansion) -->
        <!--
           Kanji numerics is normalized(converted) to Arabic numerics to maximize
           recall and reduce the synonym entries.
           Kanji numerics will be expanded, which Arabic numerics will be stored along
           with Kanji(Original) numeric expression.
        -->
	<!--
        <filter class="jp.co.rakuten.gsp.solr.analysis.GSPKanjiNumberNormalizerFactory"
                expand="true" config="kanji_number_config.properties" />
	-->

        <!-- Remove punctuation -->
        <filter class="solr.JapanesePunctuationFilterFactory" enablePositionIncrements="true"/>
        <!-- Apply Katakana stem filter -->
        <filter class="solr.JapaneseKatakanaStemFilterFactory"/>
        <!-- Remove duplicate tokens and convert them to lower case -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>       
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <!-- Rakuten character normalization -->
        <charFilter class="solr.MappingCharFilterFactory"
                    mapping="rakuten-universal-punctuation-mapping.txt"/>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>

        <!-- Japanese morphological analyzer -->
        <tokenizer class="solr.JapaneseTokenizerFactory"/>
        <!-- Apply width filter -->
        <filter class="solr.JapaneseWidthFilterFactory"/>

        <!-- Apply Kanji numeric normalizer (reduction) -->
	<!--
        <filter class="jp.co.rakuten.gsp.solr.analysis.GSPKanjiNumberNormalizerFactory"
                expand="false" config="kanji_number_config.properties" />
	-->

        <!-- Remove punctuation -->
        <filter class="solr.JapanesePunctuationFilterFactory" enablePositionIncrements="true"/>
        <!-- Apply Katakana stem filter -->
        <filter class="solr.JapaneseKatakanaStemFilterFactory"/>
        <!-- Remove duplicate tokens and convert tokens to lower case -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>       
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Japanese (Preserve punctuations) -->
    <fieldType name="text_ja_p" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- Rakuten character normalization -->
        <charFilter class="solr.MappingCharFilterFactory"
                    mapping="rakuten-universal-punctuation-mapping.txt"/>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>

        <!-- Japanese morphological analyzer -->
        <tokenizer class="solr.JapaneseTokenizerFactory"/>
        <!-- Apply width filter -->
        <filter class="solr.JapaneseWidthFilterFactory"/>

        <!-- Apply Katakana stem filter -->
        <filter class="solr.JapaneseKatakanaStemFilterFactory"/>

        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>        
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Japanese (Reading) -->
    <fieldType name="text_ja_r" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- Rakuten character normalization -->
        <charFilter class="solr.MappingCharFilterFactory"
                    mapping="rakuten-tok-punctuation-mapping.txt"/>
        <!-- Normalize input text -->
        <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                    name="nfkc"
                    mode="compose"/>

        <!-- Japanese morphological analyzer -->
        <tokenizer class="solr.JapaneseTokenizerFactory"/>
        <!-- Apply width filter -->
        <filter class="solr.JapaneseWidthFilterFactory"/>

        <!-- Remove punctuation -->
        <filter class="solr.JapanesePunctuationFilterFactory" enablePositionIncrements="true"/>

        <!-- Retreive Readings information -->
        <filter class="jp.co.rakuten.gsp.solr.analysis.GSPPronunciationFilterFactory" pronunciation-type="readings"/>
        <!-- Convert Hiragana to Katakana -->
        <filter class="jp.co.rakuten.gsp.solr.analysis.GSPHiraganaKatakanaFilterFactory"/>
        <!-- Concatenate readings to make one token -->
        <filter class="jp.co.rakuten.gsp.solr.analysis.GSPConcatFilterFactory" concat_type="katakana" />

        <!-- Apply Katakana stem filter -->
        <filter class="solr.JapaneseKatakanaStemFilterFactory"/>
        <!-- Apply N-Gram filter to create N-Gram token -->
        <filter class="jp.co.rakuten.gsp.solr.BigramFilterFactory" />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>
 
    <!-- A text field that uses for phonetic search (Experimental). -->
    <fieldtype name="phonetic" stored="false" indexed="true" class="solr.TextField" >
      <analyzer>
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/>
      </analyzer>
    </fieldtype>

    <!-- A text field that uses for storing data into payload -->
    <fieldtype name="payloads" stored="false" indexed="true" class="solr.TextField" >
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.DelimitedPayloadTokenFilterFactory" encoder="float"/>
      </analyzer>
    </fieldtype>

    <!-- since fields of this type are by default not stored or indexed,
         any data added to them will be ignored outright.  --> 
    <fieldtype name="ignored" stored="false" indexed="false" multiValued="true" class="solr.StrField" />

    <!-- The following field types are experimental. Please keep them for further
         testing and future use.
    -->
    <!-- This point type indexes the coordinates as separate fields (subFields)
         If subFieldType is defined, it references a type, and a dynamic field
         definition is created matching *___<typename>.  Alternately, if 
         subFieldSuffix is defined, that is used to create the subFields.
         Example: if subFieldType="double", then the coordinates would be
         indexed in fields myloc_0___double,myloc_1___double.
         Example: if subFieldSuffix="_d" then the coordinates would be indexed
         in fields myloc_0_d,myloc_1_d
         The subFields are an implementation detail of the fieldType, and end
         users normally should not need to know about them.
     -->
    <fieldType name="point" class="solr.PointType" dimension="2" subFieldSuffix="_d"/>

    <!-- A specialized field for geospatial search. If indexed, this fieldType must not be multivalued. -->
    <fieldType name="location" class="solr.LatLonType" subFieldSuffix="_coordinate"/>

    <!-- A Geohash is a compact representation of a latitude longitude pair in a single field. -->
    <fieldtype name="geohash" class="solr.GeoHashField"/>


 <!--
    Field type definitions for 
        - delim-type
        - text_ja_s
        - text_2gram_s
    fields
 -->
<#list fieldTypes as fieldType>
    <#if fieldType.isText_delim() >
    <fieldType name="${fieldType.name}" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.PatternTokenizerFactory" pattern="${fieldType.getOption("pattern")}" />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>
    </#if>
    <#if fieldType.isText_ja_s() >

      <fieldType name="text_ja_s" class="solr.TextField" positionIncrementGap="100">
        <analyzer type="index">
          <!-- Rakuten character normalization -->
          <charFilter class="solr.MappingCharFilterFactory"
                      mapping="rakuten-tok-punctuation-mapping.txt"/>
          <!-- Normalize input text -->
          <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                      name="nfkc"
                      mode="compose"/>

          <!-- Japanese morphological analyzer -->
          <tokenizer class="solr.JapaneseTokenizerFactory"/>
          <!-- Apply width filter -->
          <filter class="solr.JapaneseWidthFilterFactory"/>

          <!-- Apply Kanji numeric normalizer (expansion) -->
	  <!--
          <filter class="jp.co.rakuten.gsp.solr.analysis.GSPKanjiNumberNormalizerFactory"
                  expand="true" config="kanji_number_config.properties" />
	  -->

          <!-- Remove punctuations -->
          <filter class="solr.JapanesePunctuationFilterFactory" enablePositionIncrements="true"/>
          <filter class="solr.JapaneseKatakanaStemFilterFactory"/>
          <!-- Remove duplicate tokens -->
          <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
          <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
        <analyzer type="query">
          <!-- Rakuten character normalization -->
          <charFilter class="solr.MappingCharFilterFactory"
                      mapping="rakuten-tok-punctuation-mapping.txt"/>
          <!-- Normalize input text -->
          <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                      name="nfkc"
                      mode="compose"/>

          <!-- Japanese morphological analyzer -->
          <tokenizer class="solr.JapaneseTokenizerFactory"/>

          <!-- Apply query-side synonym expansion -->
          <filter class="jp.co.rakuten.gsp.solr.analysis.GSPSynonymFilterFactory" 
                  expand="true"
                  enableDynamicUpdate="true"
                  tokenizerFactory="solr.JapaneseTokenizerFactory"
		  shard_type="node_searchAPI"
                  source_type="dapcy"
		  resource_location="${fieldType.getOption("resource_location")}" />

          <!-- Apply width filter 
               Incoming query will be normalzied by ICU charfilter. However, Expanded 
               tokens by synonym filter, which are registered by dictionary team, is not
               normalized. We intend to normalize those tokens in here.               
          -->
          <filter class="solr.JapaneseWidthFilterFactory"/>

          <!-- Apply Kanji numeric normalizer (reduction) -->
	  <!--
          <filter class="jp.co.rakuten.gsp.solr.analysis.GSPKanjiNumberNormalizerFactory"
                  expand="false" config="kanji_number_config.properties" />
	  -->

          <!-- Remove punctuations -->
          <filter class="solr.JapanesePunctuationFilterFactory" enablePositionIncrements="true"/>
          <filter class="solr.JapaneseKatakanaStemFilterFactory"/>

          <!-- We DO NOT remove duplicate token in here. Because our synonym filter adds SYNONYM
               token type and sequence number to each tokens that are expanded by the filter. And then
               Our query parser uses the token type and sequence number to recognize which token
               should be a phrase or a single token. We will lost those information if we remove 
               duplicates. So we do not use RemoveDuplicatesTokenFilterFactory with synonym filter.
            -->
          <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
      </fieldType>
    </#if>
    
    <#if fieldType.isText_2gram_s() >
      <fieldType name="text_2gram_s" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true">
        <analyzer type="index">
          <!-- Rakuten character normalization -->
          <charFilter class="solr.MappingCharFilterFactory"
                      mapping="rakuten-2gram-punctuation-mapping.txt"/>
          <!-- Normalize input text -->
          <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                      name="nfkc"
                      mode="compose"/>

          <!-- Special bigram tokenizer which adds end mark to end of value. -->
          <!-- You can specify  maxLength (default: 1024) -->
          <tokenizer class="jp.co.rakuten.gsp.solr.BigramIndexTokenizerFactory" />

          <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
        <analyzer type="query">
          <!-- Rakuten character normalization -->
          <charFilter class="solr.MappingCharFilterFactory"
                      mapping="rakuten-2gram-punctuation-mapping.txt"/>
          <!-- Normalize input text -->
          <charFilter class="jp.co.rakuten.gsp.solr.ICUCharFilterFactory"
                      name="nfkc"
                      mode="compose"/>

          <!-- query side should not add end mark -->
          <tokenizer class="jp.co.rakuten.gsp.solr.BigramQueryTokenizerFactory"/>      

          <!-- Apply query-side synonym expansion -->
          <filter class="jp.co.rakuten.gsp.solr.analysis.GSPSynonymFilterFactory" 
                  expand="true"
                  enableDynamicUpdate="true"
                  tokenizerFactory="jp.co.rakuten.gsp.solr.BigramQueryTokenizerFactory"
		  shard_type="node_searchAPI"
                  source_type="dapcy"
		  resource_location="${fieldType.getOption("resource_location")}" />

          <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
      </fieldType>
    </#if>
</#list>

 </types>

 <!--
    Field definitions for the application
 -->
 <fields>
    <!-- generated fields (START)-->
<#list fields as field>
        <field name="${field.name}" type="${field.type}" indexed="${field.indexed?string}" stored="${field.stored?string}" <#if field.required >required="true"</#if> <#if field.omitNorms >omitNorms="true"</#if> <#if field.multivalue >multiValued="true"</#if>/>
</#list>
    <!-- generated fields (END)-->

   <!-- Please keep the following field definitions for future use or
        experimental to improve search capabilities and relecancy.
   -->

   <!-- Field definitions for query boost information that are generated by 
        analyzing user query and behavior.
     -->
   <field name="_query_boost_terms" type="text_ja"
          indexed="true" stored="true" omitNorms="true" multiValued="true"/>
   <!--
   <field name="_query_boost_terms_exact" type="text_kw"
          indexed="true" stored="false" omitNorms="true" multiValued="true"/>
   -->
   <field name="_query_boost_terms_exact" type="text_ws"
          indexed="true" stored="false" omitNorms="true" multiValued="true"/>

   <field name="_doc_boost" type="tfloat" indexed="true" stored="true"/>

   <!-- Dapcy is required the following two fields -->
   <!-- A field definition for storing update date, which is written by Dapcy -->
   <field name="_update_date" type="tdate" indexed="true" stored="true"/>
   <!-- A field definitions for storing collection name, which is written by Dapcy -->
   <field name="_collection_name" type="string" indexed="true" stored="false"/>

   <!-- Payload for simple query boost -->
   <field name="payloads" type="payloads" indexed="true" stored="true"/>

   <!-- Dynamic field definitions. -->
   <dynamicField name="*_s"  type="string"  indexed="true"  stored="true"/>
   <dynamicField name="*_b"  type="bool" indexed="true"  stored="true"/>

   <!-- Type used to index the lat and lon components for the "location" FieldType -->
   <dynamicField name="*_coordinate"  type="tdouble" indexed="true"  stored="false"/>

   <dynamicField name="*_p"  type="location" indexed="true" stored="true"/>

   <!-- some trie-coded dynamic fields for faster range queries -->
   <dynamicField name="*_ti" type="tint"    indexed="true"  stored="true"/>
   <dynamicField name="*_tl" type="tlong"   indexed="true"  stored="true"/>
   <dynamicField name="*_tf" type="tfloat"  indexed="true"  stored="true"/>
   <dynamicField name="*_td" type="tdouble" indexed="true"  stored="true"/>
   <dynamicField name="*_tdt" type="tdate"  indexed="true"  stored="true"/>

   <dynamicField name="ignored_*" type="ignored" multiValued="true"/>

 </fields>

 <!-- Field to use to determine and enforce document uniqueness. 
      Unless this field is marked with required="false", it will be a required field
 -->
<!-- generated uniqueKey -->
<uniqueKey>${unique.key}</uniqueKey>


 <!-- field for the QueryParser to use when an explicit fieldname is absent -->
<!-- generated default search field -->
<defaultSearchField>${unique.key}</defaultSearchField>

 <!-- SolrQueryParser configuration: defaultOperator="AND|OR" -->
 <solrQueryParser defaultOperator="AND"/>

 <!-- copyField commands are written here -->
 <copyField source="_query_boost_terms" dest="_query_boost_terms_exact"/>
 
 <!-- generated copyFields -->
<#list copyFields as copyField>
    <copyField source="${copyField.source}" dest="${copyField.dest}" />
</#list>

</schema>
